---
title: "Big Data Analytics"
subtitle: 'Lecture 7:<br>Cloud Computing'
author: "Prof. Dr. Ulrich Matter"
date: "15/04/2019"
output:   
  ioslides_presentation:
    css: ../../style/ioslides.css
logo: ../img/logo.png
bibliography: ../references/bigdata.bib
---



```{r set-options, echo=FALSE, cache=FALSE, purl=FALSE}
options(width = 100)
library(knitr)
```

# Updates

## Schedule {.smaller}

 1. Introduction: Big Data, Data Economy (Concepts). M: Walkowiak (2016): Chapter 1
 2. Programming with Data, R Refresher Course (Concepts/Applied). M: Walkowiak (2016): Chapter 2
 3. Computation and Memory (Concepts)
 4. Cleaning and Transformation of Big Data (Applied). M: Walkowiak (2016): Chapter 3: p. 74‐118.
 5. Aggregation and Visualization (Applied: data tables, ggplot). M: Walkowiak (2016): Chapter 3: p. 118‐127. C: Wickham et al. (2015), Schwabish (2014).
 6. Data Storage, Databases Interaction with R. M: Walkowiak (2016): Chapter 5.
 7. *Distributed Systems, MapReduce/Hadoop with R (Concepts/Applied). M: Walkowiak (2016): Chapter 4.*
 
 
 

# Cloud Services for Big Data Analytics

## Wrap-up: efficient use of CPU, RAM, Mass Storage

&nbsp;

```{r cpu2, echo=FALSE, out.width = "30%", fig.align='center', purl=FALSE}
include_graphics("../img/03_cpu.jpg")
```

Computationally intense tasks: parallelization, using several CPU cores (nodes) in parallel.



## Wrap-up: efficient use of CPU, RAM, Mass Storage

&nbsp;


```{r ram2, echo=FALSE, out.width = "45%", fig.align='center', purl=FALSE}
include_graphics("../img/03_ram.jpg")
```

Memory-intense tasks (data still fits into RAM): efficient memory allocation (`data.table`-package).


## Wrap-up: efficient use of CPU, RAM, Mass Storage

&nbsp;
```{r, echo=FALSE,out.width="49%", out.height="20%",fig.cap="RAM and Harddrive",fig.show='hold',fig.align='center'}
include_graphics(c("../img/03_ram.jpg", "../img/03_harddrive.jpg"))
```

Memory-intense tasks (data does not fit into RAM): efficient use of virtual memory (use parts of mass storage device as virtual memory).

## Wrap-up: efficient use of CPU, RAM, Mass Storage

```{r harddrive3, echo=FALSE, out.width = "45%", fig.align='center', purl=FALSE}
include_graphics("../img/03_harddrive.jpg")
```

(Big) Data storage: efficient storage (avoid redundancies) and efficient access (speed) with RDBMSs (here: SQLite).


## Already using all components most efficiently?

- *Scale up ('vertical scaling')*
- *Scale out ('horizontal scaling')*

## 'The Cloud'


```{r cloud, echo=FALSE, out.width = "70%", fig.align='center', purl=FALSE}
include_graphics("../img/07_cloud.png")
```





## The Cloud: Scaling Up


```{r scaleup, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```


## The Cloud: Scaling Up


```{r scaleup2, echo=FALSE, out.width = "50%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```


## The Cloud: Scaling Up


```{r scaleup3, echo=FALSE, out.width = "50%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```

- Parallel computing, large in-memory computation, SQL/NoSQL databases, etc.
- Common in scientific computing.



## The Cloud: Scaling Out


```{r scaleout, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```


```{r scaleout2, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```


```{r scaleout3, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```


## The Cloud: Scaling Out


```{r scaleout4, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```


```{r scaleout5, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```


```{r scaleout6, echo=FALSE, out.width = "20%", fig.align='center', purl=FALSE}
include_graphics("../img/07_server.png")
```

 - MapReduce/Hadoop etc.
 - Rather rare in an applied econometrics setting.


## The Cloud in Practice

Rent (virtual) machines on a flexible basis (hourly rate, etc.) from a cloud computing provider.

 - [Amazon Web Services (AWS)](https://aws.amazon.com/)
 - [Microsoft Azure](https://azure.microsoft.com/en-us/)
 - [Google Cloud Platform](https://cloud.google.com/)
 - [IBM Cloud](https://www.ibm.com/cloud/)
 - [Alibaba Cloud（阿里云)](https://www.alibabacloud.com/)
 - [Tencent Cloud (腾讯云)](https://intl.cloud.tencent.com/)
 - ...
 



# Scaling up in the Cloud

## Set up

- See the online chapter to @walkowiak_2016 ['Pushing R Further'](https://www.packtpub.com/sites/default/files/downloads/5396_6457OS_PushingRFurther.pdf) for how to set up an AWS account and the basics for how to set up AWS instances. 
- The examples below are based on the assumption that the EC2 instance and RStudio Server have been set up exactly as explained in ['Pushing R Further'](https://www.packtpub.com/sites/default/files/downloads/5396_6457OS_PushingRFurther.pdf), pages 22-38.


## Parallelization with an EC2 instance

Run non-parallelized implementation in the cloud.

```{r eval=FALSE}
# CASE STUDY: PARALLEL ---------------------------


# NOTE: the default EC2 AMI instance uses a newer compiler which data.table does not like, 
# before you can install data.table, switch to the terminal in your current RStudio Server session and 
# type the following:
# mkdir ~/.R
# echo "CC=gcc64" >> ~/.R/Makevars
# this sets the default to an older C compiler. 
# See https://stackoverflow.com/questions/48576682/r-and-data-table-on-aws for details.

# install packages
install.packages("data.table")
install.packages("doSNOW")

# load packages
library(data.table)


## ------------------------------------------------------------------------
stopdata <- read.csv("https://vincentarelbundock.github.io/Rdatasets/csv/carData/MplsStops.csv")

## ------------------------------------------------------------------------
# remove incomplete obs
stopdata <- na.omit(stopdata)
# code dependent var
stopdata$vsearch <- 0
stopdata$vsearch[stopdata$vehicleSearch=="YES"] <- 1
# code explanatory var
stopdata$white <- 0
stopdata$white[stopdata$race=="White"] <- 1

## ------------------------------------------------------------------------
model <- vsearch ~ white + factor(policePrecinct)

## ------------------------------------------------------------------------
fit <- lm(model, stopdata)
summary(fit)


# bootstrapping: normal approach

## ----message=FALSE-------------------------------------------------------

# set the 'seed' for random numbers (makes the example reproducible)
set.seed(2)

# set number of bootstrap iterations
B <- 50
# get selection of precincts
precincts <- unique(stopdata$policePrecinct)
# container for coefficients
boot_coefs <- matrix(NA, nrow = B, ncol = 2)
# draw bootstrap samples, estimate model for each sample
for (i in 1:B) {
  
  # draw sample of precincts (cluster level)
  precincts_i <- sample(precincts, size = 5, replace = TRUE)
  # get observations
  bs_i <- lapply(precincts_i, function(x) stopdata[stopdata$policePrecinct==x,])
  bs_i <- rbindlist(bs_i)
  
  # estimate model and record coefficients
  boot_coefs[i,] <- coef(lm(model, bs_i))[1:2] # ignore FE-coefficients
}

## ------------------------------------------------------------------------
se_boot <- apply(boot_coefs, 
                 MARGIN = 2,
                 FUN = sd)
se_boot



```

## Parallelization with an EC2 instance

Scaling up: rent a machine with more CPU cores.

```{r eval=FALSE}
parallel::detectCores()
```

 - EC2 instances of type `t2.micro` (free tier) only have one core. 
 - However, there are many options to scale this up (rent a machine with more CPU cores).

## Parallelization with an EC2 instance

Run the parallelized implementation on an EC2 instance.

```{r eval=FALSE}

# bootstrapping: parallel approaach

## ----message=FALSE-------------------------------------------------------
# install.packages("doSNOW", "parallel")
# load packages for parallel processing
library(doSNOW)

# get the number of cores available
ncores <- parallel::detectCores()
# set cores for parallel processing
ctemp <- makeCluster(ncores) # 
registerDoSNOW(ctemp)


# set number of bootstrap iterations
B <- 50
# get selection of precincts
precincts <- unique(stopdata$policePrecinct)
# container for coefficients
boot_coefs <- matrix(NA, nrow = B, ncol = 2)

# bootstrapping in parallel
boot_coefs <- 
  foreach(i = 1:B, .combine = rbind, .packages="data.table") %dopar% {
    
    # draw sample of precincts (cluster level)
    precincts_i <- sample(precincts, size = 5, replace = TRUE)
    # get observations
    bs_i <- lapply(precincts_i, function(x) stopdata[stopdata$policePrecinct==x,])
    bs_i <- rbindlist(bs_i)
    
    # estimate model and record coefficients
    coef(lm(model, bs_i))[1:2] # ignore FE-coefficients
    
  }


# be a good citizen and stop the snow clusters
stopCluster(cl = ctemp)



## ------------------------------------------------------------------------
se_boot <- apply(boot_coefs, 
                 MARGIN = 2,
                 FUN = sd)
se_boot



```


## Mass Storage: SQL on an EC2 instance

 - SQLite: already there! 
 - However, for the cloud a more sophisticated (client/server) SQL version makes more sense.

 
## Mass Storage: MariaDB on an EC2 instance

- For most of the installation steps, see @walkowiak_2016 (Chapter 5: 'MariaDB with R on a Amazon EC2 instance, pages 255ff). 
- However, since some of the steps shown in the book are outdated, the example below hints to some alternative/additional steps needed to make the database run on an Ubuntu 18.04 machine.
- `economics.csv` used in the local SQLite examples of Lecture 6.

## Data upload (server-side)


```{bash eval= FALSE}
# from the directory where the key-file is stored...
scp -r -i "mariadb_ec2.pem" ~/Desktop/economics.csv umatter@ec2-184-72-202-166.compute-1.amazonaws.com:~/
```

## Data import (server-side)


```{sql eval = FALSE, purl=FALSE}
-- Create the new table
CREATE TABLE econ(
date DATE,
pce REAL,
pop INTEGER,
psavert REAL,
uempmed REAL,
unemploy INTEGER
);

```

## Data import (server-side)

```{sql eval = FALSE, purl=FALSE}
LOAD DATA LOCAL INFILE
'/home/umatter/economics.csv' 
INTO TABLE econ
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;
```

## Connect to MariaDB from RStudio Server (client-side)

```{R eval= FALSE}
# load packages
library(RMySQL)

# connect to the db
con <- dbConnect(RMySQL::MySQL(), 
                 user = "umatter",
                 password = "Password1",
                 host = "localhost",
                 dbname = "data1")

```


## Query the database (client-side)

In our first query, we select all (`*`) variable values of the observation of January 1968.

```{r, eval = FALSE, purl=FALSE}
# define the query
query1 <- 
"
SELECT * FROM econ
WHERE date = '1968-01-01';
"
# send the query to the db and get the result
jan <- dbGetQuery(con, query1)
jan
```

```{}
#        date   pce    pop psavert uempmed unemploy
# 1 1968-01-01 531.5 199808    11.7     5.1     2878
```


## Query the database (client-side)

Now let's select all year/months in which there were more than 15 million unemployed, ordered by date.

```{r, eval = FALSE, purl=FALSE}
query2 <-
"
SELECT date FROM econ 
WHERE unemploy > 15000
ORDER BY date;
"

# send the query to the db and get the result
unemp <- dbGetQuery(con, query2)
head(unemp)
```

```{}
#         date
# 1 2009-09-01
# 2 2009-10-01
# 3 2009-11-01
# 4 2009-12-01
# 5 2010-01-01
# 6 2010-02-01

```



# Scaling Out: MapReduce/Hadoop

## MapReduce: Word Count Example

From @walkowiak_2016 (Chapter 4)

<center>
*Simon is a friend of Becky.*
&nbsp;

*Becky is a friend of Ann.*
&nbsp;

*Ann is not a friend of Simon.*
</center> 

## MapReduce: Word Count Example

```{r mapreduce1, echo=FALSE, out.width = "80%", fig.align='center', fig.cap= "Source: @walkowiak_2016, Chapter 4", purl=FALSE}
include_graphics("../img/07_mapreduce1.png")
```



## MapReduce: Word Count Example

```{r mapreduce2, echo=FALSE, out.width = "80%", fig.align='center', fig.cap= "Source: @walkowiak_2016, Chapter 4", purl=FALSE}
include_graphics("../img/07_mapreduce2.png")
```



## Map/Reduce Concept: illustration in R

 - `map()`
 - `reduce()`
 
Note: this code example serves to illustrate the underlying idea of MapReduce and how it is related to the idea of `map` and `reduce` functions. It does *not* suggest that MapReduce actually is simply an application of the classical `map` and `reduce (fold)` functions.



## Map/Reduce Concept: illustration in R

```{r}
input_text <-
"Simon is a friend of Becky.
Becky is a friend of Ann.
Ann is not a friend of Simon."
```


## Mapper


```{r}
# Mapper splits input into lines
lines <- as.list(strsplit(input_text, "\n")[[1]])
lines
```


## Mapper

```{r}

# Mapper splits lines into Key-Value pairs
map_fun <-
     function(x){
          
          # remove special characters
          x_clean <- gsub("[[:punct:]]", "", x)
          # split line into words
          keys <- unlist(strsplit(x_clean, " "))
          # initiate key-value pairs
          key_values <- rep(1, length(keys))
          names(key_values) <- keys
          
          return(key_values)
     }

kv_pairs <- Map(map_fun, lines)

# look at the result
kv_pairs
```



## Reducer


```{r}
# order and shuffle
kv_pairs <- unlist(kv_pairs)
keys <- unique(names(kv_pairs))
keys <- keys[order(keys)]
shuffled <- lapply(keys,
                    function(x) kv_pairs[x == names(kv_pairs)])
shuffled
```

## Reducer

Now we can sum up the keys in order to the the word count for the entire input.

```{r}
sums <- sapply(shuffled, sum)
names(sums) <- keys
sums
```

## Simpler example: Compute the total number of words

```{r}
# assigns the number of words per line as value
map_fun2 <- 
     function(x){
          # remove special characters
          x_clean <- gsub("[[:punct:]]", "", x)
          # split line into words, count no. of words per line
          values <- length(unlist(strsplit(x_clean, " ")))
          return(values)
     }
# Mapper
mapped <- Map(map_fun2, lines)
mapped

# Reducer
reduced <- Reduce(sum, mapped)
reduced
```



# Map/Reduce with Hadoop on an Azure VM


## References {.smaller}

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


