---
title: "Big Data Analytics"
subtitle: 'Lecture 4:<br>Cleaning and Transformation of Big Data'
author: "Prof. Dr. Ulrich Matter"
date: "11/03/2019"
output:   
  ioslides_presentation:
    css: ../../style/ioslides.css
logo: ../img/logo.png
bibliography: ../references/bigdata.bib
---



```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
library(knitr)
```


# Updates

## Register in GitHub Classroom

- Email with instructions will follow.
- Call to register Group Examination Teams!
 - If you haven't formed your team yet, do so within the next two weeks.
 
## Small Change in the Schedule {.smaller}

 1. Introduction: Big Data, Data Economy (Concepts). M: Walkowiak (2016): Chapter 1
 2. Programming with Data, R Refresher Course (Concepts/Applied). M: Walkowiak (2016): Chapter 2
 3. Computation and Memory (Concepts)
 4. Cleaning and Transformation of Big Data (Applied). M: Walkowiak (2016): Chapter 3: p. 74‐118.
 5. Aggregation and Visualization (Applied: data tables, ggplot). M: Walkowiak (2016): Chapter 3: p. 118‐127. C: Wickham et al. (2015), Schwabish (2014).
 6. *Data Storage, Databases Interaction with R. M: Walkowiak (2016): Chapter 5*
 7. *Distributed Systems, MapReduce/Hadoop with R (Concepts/Applied). M: Walkowiak (2016): Chapter 4.*
 

## R Markdown/Course Repo

- Clone the repository: see README
- R Markdown
 
# Recap Week 3
 
## Components of a computing environment
 

```{r cpu2, echo=FALSE, out.width = "25%", fig.align='center', purl=FALSE}
include_graphics("../img/03_cpu.jpg")
```

```{r ram2, echo=FALSE, out.width = "25%", fig.align='center', purl=FALSE}
include_graphics("../img/03_ram.jpg")
```

```{r harddrive2, echo=FALSE, out.width = "25%", fig.align='center', purl=FALSE}
include_graphics("../img/03_harddrive.jpg")
```


## Components of a computing environment

<center> *Why should we care?* </center>


## Big Data (Analytics)

- Need to understand how to *make best use of the available resources*, given a specific data analysis task.
     - CPU: Parallel processing (use all cores available)
     - RAM: Efficient memory allocation and usage
     - RAM + Mass Storage: Virtual memory, efficient swapping

## Insight from analyzing methods conceptually

- Methods for big data analytics come with an *'overhead'*
     - Additional 'preparatory' steps.
     - Only faster than traditional methods if data set has a certain size!

## Insight from analyzing methods conceptually

- Methods for big data analytics come with an *'overhead'*
     - Additional 'preparatory' steps.
     - Only faster than traditional methods if data set has a certain size!
- Examples: 
     - Uluru algorithm: Subsample, more steps, more lines of code than OLS.
     - Parallel processing: Distribute data/task, combine afterwards.
     - `fread`: Memory maps data before actually reading it into RAM.
     
     


## Beyond memory

 - RAM is not sufficient to handle the amount of data to be analyzed...
 - *What to do?*
 
## Beyond memory

 - RAM is not sufficient to handle the amount of data to be analyzed...
 - *What to do?*
 - Scale up by using parts of the available Mass Storage (hard-disk) as *virtual memory*
 


## Out-of-memory strategies

- Chunked data files on disk
- Memory-mapped files and shared memory

## Out-of-memory strategies

- Chunked data files on disk: `ff`-package
- Memory-mapped files and shared memory: `bigmemory`-package




## Chunking data with the `ff`-package

Preparations 
```{r message=FALSE}

# SET UP --------------

# install.packages(c("ff", "ffbase"))
# load packages
library(ff)
library(ffbase)
library(pryr)

# create directory for ff chunks, and assign directory to ff 
system("mkdir ffdf")
options(fftempdir = "ffdf")

```


## Chunking data with the `ff`-package

Import data, inspect change in RAM.

```{r echo=FALSE, message=FALSE, warning=FALSE}
gc()
```


```{r}
mem_change(
flights <- 
     read.table.ffdf(file="../data/flights.csv",
                     sep=",",
                     VERBOSE=TRUE,
                     header=TRUE,
                     next.rows=100000,
                     colClasses=NA)
)
```


## Chunking data with the `ff`-package

Inspect file chunks on disk and data structure in R environment.

```{r}
# show the files in the directory keeping the chunks
list.files("ffdf")

# investigate the structure of the object created in the R environment
summary(flights)
```



## Memory mapping with `bigmemory`

Preparations

```{r message=FALSE}

# SET UP ----------------

# load packages
library(bigmemory)
library(biganalytics)
```



## Memory mapping with `bigmemory`

Import data, inspect change in RAM.

```{r}
# import the data
flights <- read.big.matrix("../data/flights.csv",
                     type="integer",
                     header=TRUE,
                     backingfile="flights.bin",
                     descriptorfile="flights.desc")
```


## Memory mapping with `bigmemory`

Inspect the imported data.

```{r}
summary(flights)
```


## Memory mapping with `bigmemory`

Inspect the object loaded into the R environment.

```{r}
flights
```


## Memory mapping with `bigmemory`

- `backingfile`: The cache for the imported file (holds the raw data on disk).
- `descriptorfile`: Metadata describing the imported data set (also on disk).


## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

First, import a large data set without a backing-file:

```{r}
# import data and check time needed  
system.time(
     flights1 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer")
)

# import data and check memory used
mem_change(
     flights1 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer")
)

flights1 
```




## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

Second, import the same data set with a backing-file:

```{r}
# import data and check time needed  
system.time(
     flights2 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer",
                                 backingfile = "flights2.bin",
                                 descriptorfile = "flights2.desc"
                                 )
)

# import data and check memory used
mem_change(
     flights2 <- read.big.matrix("../data/flights.csv",
                                 header = TRUE,
                                 sep = ",",
                                 type = "integer",
                                 backingfile = "flights2.bin",
                                 descriptorfile = "flights2.desc"
                                 )
)

flights2
```


## Memory mapping with `bigmemory`

Understanding the role of `backingfile` and `descriptorfile`.

Third, re-import the same data set with a backing-file.

```{r}
# remove the loaded file
rm(flights2)

# 'load' it via the backing-file
system.time(flights2 <- attach.big.matrix("flights2.desc"))

flights2

```





# Cleaning and Transformation 


## Typical tasks (independent of data set size)

- Normalize/standardize.
- Code additional variables (indicators, strings to categorical, etc.).
- Remove, add covariates.
- Merge data sets.
- Set data types.

## Typical workflow

1. Import raw data.
2. Clean/transform.
3. Store for analysis.
     - Write to file.
     - Write to database.
     
## Bottlenecks

- RAM:
     - Raw data does not fit into memory.
     - Transformations enlarge RAM allocation (copying).
- Mass Storage: Reading/Writing
- CPU: Parsing (data types)

# Data Preparation with `ff`

## Set up

The following examples are based on @walkowiak_2016, Chapter 3.

```{r warning=FALSE}

## SET UP ------------------------

#Set working directory to the data and airline_id files.
# setwd("materials/code_book/B05396_Ch03_Code")
system("mkdir ffdf")
options(fftempdir = "ffdf")

# load packages
library(ff)
library(ffbase)
library(pryr)

# fix vars
FLIGHTS_DATA <- "../code_book/B05396_Ch03_Code/flights_sep_oct15.txt"
AIRLINES_DATA <- "../code_book/B05396_Ch03_Code/airline_id.csv"

```

## Data import

```{r}

# DATA IMPORT ------------------

# 1. Upload flights_sep_oct15.txt and airline_id.csv files from flat files. 

system.time(flights.ff <- read.table.ffdf(file=FLIGHTS_DATA,
                                          sep=",",
                                          VERBOSE=TRUE,
                                          header=TRUE,
                                          next.rows=100000,
                                          colClasses=NA))

airlines.ff <- read.csv.ffdf(file= AIRLINES_DATA,
                             VERBOSE=TRUE,
                             header=TRUE,
                             next.rows=100000,
                             colClasses=NA)
# check memory used
mem_used()

```


## Comparison with `read.table`

```{r}

##Using read.table()
system.time(flights.table <- read.table(FLIGHTS_DATA, 
                                        sep=",",
                                        header=TRUE))

gc()

system.time(airlines.table <- read.csv(AIRLINES_DATA,
                                       header = TRUE))


# check memory used
mem_used()

```


## Inspect imported files

```{r}
# 2. Inspect the ffdf objects.
## For flights.ff object:
class(flights.ff)
dim(flights.ff)
## For airlines.ff object:
class(airlines.ff)
dim(airlines.ff)

```

## Data cleaning and transformation

Goal: merge airline data to flights data


```{r}
# step 1: 
## Rename "Code" variable from airlines.ff to "AIRLINE_ID" and "Description" into "AIRLINE_NM".
names(airlines.ff) <- c("AIRLINE_ID", "AIRLINE_NM")
names(airlines.ff)
str(airlines.ff[1:20,])
```


## Data cleaning and transformation

Goal: merge airline data to flights data

```{r}
# merge of ffdf objects
mem_change(flights.data.ff <- merge.ffdf(flights.ff, airlines.ff, by="AIRLINE_ID"))
#The new object is only 551.2 Kb in size
class(flights.data.ff)
dim(flights.data.ff)
dimnames.ffdf(flights.data.ff)
```

## Inspect difference to in-memory operation

```{r}
##For flights.table:
names(airlines.table) <- c("AIRLINE_ID", "AIRLINE_NM")
names(airlines.table)
str(airlines.table[1:20,])

# check memory usage of merge in RAM 
mem_change(flights.data.table <- merge(flights.table,
                                       airlines.table,
                                       by="AIRLINE_ID"))
#The new object is already 105.7 Mb in size
#A rapid spike in RAM use when processing
```

## Type conversion: ff factor

```{r}

# Inspect the current variable
table.ff(flights.data.ff$DAY_OF_WEEK)
head(flights.data.ff$DAY_OF_WEEK)

# Convert numeric ff DAY_OF_WEEK vector to a ff factor:
flights.data.ff$WEEKDAY <- cut.ff(flights.data.ff$DAY_OF_WEEK, 
                                   breaks = 7, 
                                   labels = c("Monday", "Tuesday", 
                                              "Wednesday", "Thursday", 
                                              "Friday", "Saturday",
                                              "Sunday"))
# inspect the result
head(flights.data.ff$WEEKDAY)
table.ff(flights.data.ff$WEEKDAY)

```



## Subsetting

```{r}
mem_used()

# Subset the ffdf object flights.data.ff:
subs1.ff <- subset.ffdf(flights.data.ff, CANCELLED == 1, 
                        select = c(FL_DATE, AIRLINE_ID, 
                                   ORIGIN_CITY_NAME,
                                   ORIGIN_STATE_NM,
                                   DEST_CITY_NAME,
                                   DEST_STATE_NM,
                                   CANCELLATION_CODE))

dim(subs1.ff)
mem_used()

```


## Save to ffdf-files
(For further processing with `ff`)

```{r}
# Save a newly created ffdf object to a data file:

save.ffdf(subs1.ff) #7 files (one for each column) created in the ffdb directory

```


## Load ffdf-files

```{r}
# Loading previously saved ffdf files:
rm(subs1.ff)
gc()
load.ffdf("ffdb")
str(subs1.ff)
dim(subs1.ff)
dimnames(subs1.ff)
```

## Export to CSV

```{r message=FALSE}
#  Export subs1.ff into CSV and TXT files:
write.csv.ffdf(subs1.ff, "subset1.csv")

```



## References 




```{r echo=FALSE, message = FALSE, warning=FALSE}
# clean up
system("rm -r ffdf")
system("rm -r ffdb")
system("rm flights.bin")
system("rm flights.desc")
system("rm flights2.bin")
system("rm flights2.desc")
system("rm subset1.csv")

```


